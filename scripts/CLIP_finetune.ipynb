{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a635753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import random\n",
    "import tqdm\n",
    "import open_clip\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d296a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_path = os.path.dirname(os.getcwd())\n",
    "data_root = os.path.join(proj_path, 'rare_animals_dataset')\n",
    "img_dir = os.path.join(data_root, 'images')\n",
    "description_dir = os.path.join(data_root, 'definitions.json')\n",
    "saved_model_dir = os.path.join(proj_path, \"weights\", \"fine-tuned\", \"clip_finetuned.pt\")\n",
    "clip_pretrained = os.path.join(proj_path, \"weights\", \"pretrained\", \"biotroveclip-vit-b-16-from-openai-epoch-40.pt\")\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 10\n",
    "LR = 2e-5\n",
    "CLIP_MODEL_NAME = \"ViT-B-16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71a61e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAnimalDataset(Dataset):\n",
    "    def __init__(self, image_dir, description_file, preprocess_fn, split='train'):\n",
    "        self.image_dir = image_dir\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        self.data_pairs = []\n",
    "\n",
    "        with open(description_file, 'r', encoding='utf-8') as f:\n",
    "            self.animal_descriptions = json.load(f)\n",
    "\n",
    "        self.all_species_names = sorted(list(self.animal_descriptions.keys()))\n",
    "        self.species_name_to_idx = {name: i for i, name in enumerate(self.all_species_names)}\n",
    "\n",
    "        all_labels_in_folders = [d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))]\n",
    "\n",
    "        for label_folder_name in all_labels_in_folders:\n",
    "            label_folder_path = os.path.join(image_dir, label_folder_name)\n",
    "            if label_folder_name not in self.animal_descriptions:\n",
    "                continue\n",
    "\n",
    "            english_name = label_folder_name\n",
    "            details = self.animal_descriptions[english_name]\n",
    "            scientific_name = details.get('scientific', '')\n",
    "            description_text = details.get('description', '')\n",
    "            species_prompts = []\n",
    "            species_prompts.append(f\"a photo of a {english_name}\")\n",
    "            if scientific_name:\n",
    "                species_prompts.append(f\"a photo of a {scientific_name}\")\n",
    "            if description_text:\n",
    "                species_prompts.append(f\"a photo of a {english_name}, {description_text}\")\n",
    "            species_prompts = list(set(species_prompts))\n",
    "            #print (species_prompts)\n",
    "\n",
    "            for img_name in os.listdir(label_folder_path):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    img_path = os.path.join(label_folder_path, img_name)\n",
    "                    try:\n",
    "                        Image.open(img_path).convert(\"RGB\").close()\n",
    "                    except Exception as e:\n",
    "                        print(f\"ERROR: Could not open image {img_path}: {e}. Skipping.\")\n",
    "                        continue\n",
    "                    self.data_pairs.append((img_path, species_prompts, english_name))\n",
    "\n",
    "        if split == 'train':\n",
    "            transform_set = T.Compose(\n",
    "            \n",
    "            )\n",
    "            self.current_data, _ = train_test_split(self.data_pairs, test_size=0.2, random_state=42)\n",
    "        elif split == 'val':\n",
    "            _, self.current_data = train_test_split(self.data_pairs, test_size=0.2, random_state=42)\n",
    "        else:\n",
    "            raise ValueError(\"Split must be 'train' or 'val'\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.current_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, text_prompts_list, english_name = self.current_data[idx]\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.preprocess_fn(image)\n",
    "\n",
    "        chosen_text_prompt = random.choice(text_prompts_list)\n",
    "        text_tokens = open_clip.tokenize(chosen_text_prompt)\n",
    "\n",
    "        ground_truth_global_idx = self.species_name_to_idx[english_name]\n",
    "        return image, text_tokens, ground_truth_global_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f37fb64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "BioTrove-CLIP model loaded.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    model_name=CLIP_MODEL_NAME,\n",
    "    pretrained=None,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "full_checkpoint = torch.load(clip_pretrained, map_location=device, weights_only=False)\n",
    "pretrained_state_dict = full_checkpoint['state_dict']\n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in pretrained_state_dict.items():\n",
    "    if k.startswith('module.'):\n",
    "        new_state_dict[k[7:]] = v\n",
    "    else:\n",
    "        new_state_dict[k] = v\n",
    "model.load_state_dict(new_state_dict)\n",
    "print(\"BioTrove-CLIP model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2da2d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dataset = CustomAnimalDataset(img_dir, description_dir, preprocess, split='train')\n",
    "all_species_names = temp_dataset.all_species_names\n",
    "species_name_to_idx = temp_dataset.species_name_to_idx\n",
    "\n",
    "all_eval_text_prompts = [f\"a photo of a {name}\" for name in all_species_names]\n",
    "#print(f\"Encoding {len(all_eval_text_prompts)} prompts for evaluation...\")\n",
    "with torch.no_grad():\n",
    "    all_text_tokens_for_eval = open_clip.tokenize(all_eval_text_prompts).to(device)\n",
    "    all_eval_text_features = model.encode_text(all_text_tokens_for_eval)\n",
    "    all_eval_text_features = F.normalize(all_eval_text_features, p=2, dim=-1)\n",
    "#print(\"Finished encoding all prompts for evaluation.\")\n",
    "\n",
    "train_dataset = CustomAnimalDataset(img_dir, description_dir, preprocess, split='train')\n",
    "val_dataset = CustomAnimalDataset(img_dir, description_dir, preprocess, split='val')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42de54dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 (Train): 100%|██████████| 60/60 [03:59<00:00,  3.99s/it, loss=1.26] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training Loss: 1.2560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 (Validation): 100%|██████████| 16/16 [00:12<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Val loss: 1.3216\n",
      "Epoch 1 Validation Top-1 acc: 0.2258\n",
      "Epoch 1 Validation Top-3 acc: 0.5161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 (Train): 100%|██████████| 60/60 [03:29<00:00,  3.50s/it, loss=0.685]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training Loss: 0.6852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 (Validation): 100%|██████████| 16/16 [00:15<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Val loss: 0.7887\n",
      "Epoch 2 Validation Top-1 acc: 0.2581\n",
      "Epoch 2 Validation Top-3 acc: 0.4839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 (Train): 100%|██████████| 60/60 [15:43<00:00, 15.73s/it, loss=0.573]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training Loss: 0.5727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 (Validation): 100%|██████████| 16/16 [00:25<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Val loss: 0.8825\n",
      "Epoch 3 Validation Top-1 acc: 0.2903\n",
      "Epoch 3 Validation Top-3 acc: 0.4516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 (Train): 100%|██████████| 60/60 [04:31<00:00,  4.52s/it, loss=0.454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training Loss: 0.4537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 (Validation): 100%|██████████| 16/16 [00:18<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Val loss: 0.6027\n",
      "Epoch 4 Validation Top-1 acc: 0.1935\n",
      "Epoch 4 Validation Top-3 acc: 0.5806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 (Train): 100%|██████████| 60/60 [04:52<00:00,  4.87s/it, loss=0.257] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training Loss: 0.2571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 (Validation): 100%|██████████| 16/16 [00:20<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Val loss: 0.4382\n",
      "Epoch 5 Validation Top-1 acc: 0.2258\n",
      "Epoch 5 Validation Top-3 acc: 0.4194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 (Train): 100%|██████████| 60/60 [04:11<00:00,  4.19s/it, loss=0.262]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Training Loss: 0.2625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 (Validation): 100%|██████████| 16/16 [00:14<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Val loss: 0.6155\n",
      "Epoch 6 Validation Top-1 acc: 0.1613\n",
      "Epoch 6 Validation Top-3 acc: 0.5484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 (Train): 100%|██████████| 60/60 [04:25<00:00,  4.43s/it, loss=0.594]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Training Loss: 0.5941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 (Validation): 100%|██████████| 16/16 [00:16<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Val loss: 0.3642\n",
      "Epoch 7 Validation Top-1 acc: 0.2903\n",
      "Epoch 7 Validation Top-3 acc: 0.5161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 (Train):  65%|██████▌   | 39/60 [02:39<01:30,  4.33s/it, loss=0.117] "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    train_bar = tqdm.tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} (Train)\")\n",
    "\n",
    "    for batch_idx, (images, texts_tokens, _) in enumerate(train_bar):\n",
    "        images = images.to(device)\n",
    "        texts_tokens = texts_tokens.squeeze(1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        image_features = model.encode_image(images)\n",
    "        text_features = model.encode_text(texts_tokens)\n",
    "\n",
    "        image_features = F.normalize(image_features, p=2, dim=-1)\n",
    "        text_features = F.normalize(text_features, p=2, dim=-1)\n",
    "\n",
    "        logit_scale = model.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.T\n",
    "        logits_per_text = logits_per_image.T\n",
    "\n",
    "        labels = torch.arange(len(images), device=device) # Label for contrastive loss\n",
    "        \n",
    "        loss_i = F.cross_entropy(logits_per_image, labels)\n",
    "        loss_t = F.cross_entropy(logits_per_text, labels)\n",
    "        loss = (loss_i + loss_t) / 2\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        train_bar.set_postfix(loss=total_loss / (batch_idx + 1))\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct_top1 = 0\n",
    "    val_correct_top3 = 0\n",
    "    total_samples = 0\n",
    "    val_bar = tqdm.tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} (Validation)\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, texts_tokens_for_loss, ground_truth_indices in val_bar:\n",
    "            images = images.to(device)\n",
    "            texts_tokens_for_loss = texts_tokens_for_loss.squeeze(1).to(device)\n",
    "            ground_truth_indices = ground_truth_indices.to(device)\n",
    "\n",
    "            image_features = model.encode_image(images)\n",
    "            image_features = F.normalize(image_features, p=2, dim=-1)\n",
    "\n",
    "            val_text_features = model.encode_text(texts_tokens_for_loss)\n",
    "            val_text_features = F.normalize(val_text_features, p=2, dim=-1)\n",
    "            val_logits = model.logit_scale.exp() * image_features @ val_text_features.T\n",
    "            \n",
    "            labels_val = torch.arange(len(images), device=device)\n",
    "            loss_i = F.cross_entropy(val_logits, labels_val)\n",
    "            loss_t = F.cross_entropy(val_logits.T, labels_val)\n",
    "            loss = (loss_i + loss_t) / 2\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            logits_per_image_all_classes = (100.0 * image_features @ all_eval_text_features.T)\n",
    "            \n",
    "            num_total_species = all_eval_text_features.size(0)\n",
    "            k_val = min(3, num_total_species)\n",
    "\n",
    "            if k_val == 0:\n",
    "                continue\n",
    "\n",
    "            _, top_preds = logits_per_image_all_classes.topk(k_val, dim=1)\n",
    "            \n",
    "            val_correct_top1 += (top_preds[:, 0] == ground_truth_indices).sum().item()\n",
    "            val_correct_top3 += (top_preds == ground_truth_indices.unsqueeze(1)).any(dim=1).sum().item()\n",
    "\n",
    "            total_samples += images.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc_top1 = val_correct_top1 / total_samples if total_samples > 0 else 0\n",
    "    val_acc_top3 = val_correct_top3 / total_samples if total_samples > 0 else 0\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Val loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Epoch {epoch+1} Validation Top-1 acc: {val_acc_top1:.4f}\")\n",
    "    print(f\"Epoch {epoch+1} Validation Top-3 acc: {val_acc_top3:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9fdb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model saved to biotrove_clip_finetuned.pt\n"
     ]
    }
   ],
   "source": [
    "# torch.save(model.state_dict(), SAVE_MODEL_PATH)\n",
    "# print(f\"Fine-tuned model saved to {SAVE_MODEL_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
